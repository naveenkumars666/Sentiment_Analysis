{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u-ZmLUIDIxc",
        "outputId": "6f0fb168-7987-429d-daac-a3a2d3aa55c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geKcgZGpDICY",
        "outputId": "7206a104-2b5a-4518-da09-8b5f452ee016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1 - Overall Sentiment: Negative\n",
            "Sentence 1: POSITIVE (Score: 0.9087)\n",
            "Sentence 2: NEGATIVE (Score: 0.9998)\n",
            "Sentence 3: NEGATIVE (Score: 0.9995)\n",
            "\n",
            "Review 2 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9883)\n",
            "Sentence 2: NEGATIVE (Score: 0.9997)\n",
            "Sentence 3: POSITIVE (Score: 0.9491)\n",
            "\n",
            "Review 3 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9446)\n",
            "Sentence 2: NEGATIVE (Score: 0.9966)\n",
            "Sentence 3: POSITIVE (Score: 0.7511)\n",
            "\n",
            "Review 4 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9992)\n",
            "Sentence 2: POSITIVE (Score: 0.9867)\n",
            "Sentence 3: POSITIVE (Score: 0.9999)\n",
            "\n",
            "Review 5 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9655)\n",
            "Sentence 2: POSITIVE (Score: 0.9965)\n",
            "Sentence 3: POSITIVE (Score: 0.9997)\n",
            "Sentence 4: POSITIVE (Score: 0.7511)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# List of reviews\n",
        "reviews = [\n",
        "    '''Product is good but very expensive....Company does not have proper customer support.... I am scared if they support till I will have the warranty???Better go for other brands''',\n",
        "    \"After seeing the specs of this machine, I was ordered with excitement and with expectations. But when it comes to the practical, I felt personally the washing quality is not as expected and it's not very user friendly....i will update my review once I complete 6 months\",\n",
        "    \"Best washing quality better than drycleaning as you save a lot of money. The other thing is you can scan the stain via Miraie app which gives all pre treatment details.\",\n",
        "    \"I recommend this washing machine, made our life easy in many ways. Easy to use, smart washing modes which you can customize as per your needs, saves lot of water since it understand required utilisation of water. Great product !!\",\n",
        "    \"Machine works well in all cases, less water utilization, energy power saving, Slim & Sturdy. Easy to operate, Washes the clothes properly & then drys them well enough. In short a overall good product to use.\"\n",
        "]\n",
        "\n",
        "# Initialize variables to store sentiment scores\n",
        "overall_sentiments = []\n",
        "\n",
        "# Function to tokenize and split text into sentences\n",
        "def tokenize_and_split_text(text, tokenizer):\n",
        "    # Tokenize and split text at punctuation marks\n",
        "    tokenized_sentences = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    sentences = tokenizer.decode(tokenized_sentences['input_ids'][0]).split('.')\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "# Function to calculate the overall sentiment based on sentence sentiments and scores\n",
        "def calculate_overall_sentiment(sentiments):\n",
        "    positive_scores = [score for label, score in sentiments if label == 'POSITIVE']\n",
        "    negative_scores = [score for label, score in sentiments if label == 'NEGATIVE']\n",
        "    neutral_scores = [score for label, score in sentiments if label == 'NEUTRAL']\n",
        "\n",
        "    positive_count = len(positive_scores)\n",
        "    negative_count = len(negative_scores)\n",
        "    neutral_count = len(neutral_scores)\n",
        "\n",
        "    if positive_count > negative_count and positive_count > neutral_count:\n",
        "        return \"Positive\"\n",
        "    elif negative_count > positive_count and negative_count > neutral_count:\n",
        "        return \"Negative\"\n",
        "    elif neutral_count > positive_count and neutral_count > negative_count:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# Process each review\n",
        "for review in reviews:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # You can change the tokenizer as needed\n",
        "\n",
        "    sentences = tokenize_and_split_text(review, tokenizer)\n",
        "    review_sentiments = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = sentiment_analyzer(sentence)[0]\n",
        "        label = sentiment['label']\n",
        "        score = sentiment['score']\n",
        "\n",
        "        review_sentiments.append((label, score))\n",
        "\n",
        "    overall_sentiment = calculate_overall_sentiment(review_sentiments)\n",
        "\n",
        "    overall_sentiments.append({\n",
        "        'sentiments': review_sentiments,\n",
        "        'overall_sentiment': overall_sentiment\n",
        "    })\n",
        "\n",
        "# Print the results\n",
        "for i, review_sentiment in enumerate(overall_sentiments):\n",
        "    print(f\"Review {i + 1} - Overall Sentiment: {review_sentiment['overall_sentiment']}\")\n",
        "    for j, (label, score) in enumerate(review_sentiment['sentiments']):\n",
        "        print(f\"Sentence {j + 1}: {label} (Score: {score:.4f})\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# List of reviews\n",
        "reviews = [\n",
        "    '''I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.\n",
        "     The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!''',\n",
        "    \"After seeing the specs of this machine, I was ordered with excitement and with expectations. But when it comes to the practical, I felt personally the washing quality is not as expected and it's not very user friendly....i will update my review once I complete 6 months\",\n",
        "    \"Best washing quality better than drycleaning as you save a lot of money. The other thing is you can scan the stain via Miraie app which gives all pre treatment details.\",\n",
        "    \"I recommend this washing machine, made our life easy in many ways. Easy to use, smart washing modes which you can customize as per your needs, saves lot of water since it understand required utilisation of water. Great product !!\",\n",
        "    \"Machine works well in all cases, less water utilization, energy power saving, Slim & Sturdy. Easy to operate, Washes the clothes properly & then drys them well enough. In short a overall good product to use.\"\n",
        "]\n",
        "\n",
        "# Initialize variables to store sentiment scores\n",
        "overall_sentiments = []\n",
        "\n",
        "# Function to tokenize and split text into sentences\n",
        "def tokenize_and_split_text(text, tokenizer):\n",
        "    # Tokenize and split text at punctuation marks\n",
        "    tokenized_sentences = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    sentences = tokenizer.decode(tokenized_sentences['input_ids'][0]).split('.')\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "# Function to calculate the overall sentiment based on sentence sentiments and scores\n",
        "def calculate_overall_sentiment(sentiments):\n",
        "    positive_scores = [score for label, score in sentiments if label == 'POSITIVE']\n",
        "    negative_scores = [score for label, score in sentiments if label == 'NEGATIVE']\n",
        "    neutral_scores = [score for label, score in sentiments if label == 'NEUTRAL']\n",
        "\n",
        "    positive_count = len(positive_scores)\n",
        "    negative_count = len(negative_scores)\n",
        "    neutral_count = len(neutral_scores)\n",
        "\n",
        "    if positive_count > negative_count and positive_count > neutral_count:\n",
        "        return \"Positive\"\n",
        "    elif negative_count > positive_count and negative_count > neutral_count:\n",
        "        return \"Negative\"\n",
        "    elif neutral_count > positive_count and neutral_count > negative_count:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# Process each review\n",
        "for review in reviews:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # You can change the tokenizer as needed\n",
        "\n",
        "    sentences = tokenize_and_split_text(review, tokenizer)\n",
        "    review_sentiments = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentiment = sentiment_analyzer(sentence)[0]\n",
        "        label = sentiment['label']\n",
        "        score = sentiment['score']\n",
        "\n",
        "        review_sentiments.append((label, score))\n",
        "\n",
        "    overall_sentiment = calculate_overall_sentiment(review_sentiments)\n",
        "\n",
        "    overall_sentiments.append({\n",
        "        'sentiments': review_sentiments,\n",
        "        'overall_sentiment': overall_sentiment\n",
        "    })\n",
        "\n",
        "# Print the results\n",
        "for i, review_sentiment in enumerate(overall_sentiments):\n",
        "    print(f\"Review {i + 1} - Overall Sentiment: {review_sentiment['overall_sentiment']}\")\n",
        "    for j, (label, score) in enumerate(review_sentiment['sentiments']):\n",
        "        print(f\"Sentence {j + 1}: {label} (Score: {score:.4f})\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "T2L2JSweDOCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5fd925-bcf1-46b0-ed59-fb185d5ae8e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.8503)\n",
            "Sentence 2: POSITIVE (Score: 0.9999)\n",
            "Sentence 3: NEGATIVE (Score: 0.9951)\n",
            "Sentence 4: POSITIVE (Score: 0.9999)\n",
            "\n",
            "Review 2 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9883)\n",
            "Sentence 2: NEGATIVE (Score: 0.9997)\n",
            "Sentence 3: POSITIVE (Score: 0.9491)\n",
            "\n",
            "Review 3 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9446)\n",
            "Sentence 2: NEGATIVE (Score: 0.9966)\n",
            "Sentence 3: POSITIVE (Score: 0.7511)\n",
            "\n",
            "Review 4 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9992)\n",
            "Sentence 2: POSITIVE (Score: 0.9867)\n",
            "Sentence 3: POSITIVE (Score: 0.9999)\n",
            "\n",
            "Review 5 - Overall Sentiment: Positive\n",
            "Sentence 1: POSITIVE (Score: 0.9655)\n",
            "Sentence 2: POSITIVE (Score: 0.9965)\n",
            "Sentence 3: POSITIVE (Score: 0.9997)\n",
            "Sentence 4: POSITIVE (Score: 0.7511)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SS-9rFBN1Q3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}